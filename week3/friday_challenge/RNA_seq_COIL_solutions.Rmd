---
title: "RNA-seq tutorial - Data QC"
author: "UZH n UU: the generic account"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  bookdown::html_document2: default
  bookdown::pdf_document2: default
---

## RNA-seq analysis - Count QC

RNAseq can be analysed in two ways. The first method involves mapping the reads to the reference genome with a splice-aware aligner (e.g.STAR) and the quantifying the expression of genes from the alignments e.g with featureCounts R package. The second method involves quantifying gene expression via psedualignment e.g. with Kallisto. The second method takes a lot less time due to omitting read alignment to the genome.

Here we generated for you two tables quantifying the expression of genes per a set of samples using both methods. The data is from yeast RNA sequencing. The tables are stored as rds objects which take less space. Hint: to read in the tables use readRDS function.

```{r}
suppressPackageStartupMessages({
  library(SummarizedExperiment)
  library(pheatmap)
  library(RColorBrewer)
  library(stringr)
  library(plotly)
  library(tidyverse)
  library(dplyr)
})
```

### Exercise #1

1.  Load in the tables and take a look at the first 10 rows.

    ```{r}
fc <- readRDS("featureCounts/mergedCounts.rds")
ka <- readRDS("Kallisto/mergedCounts.rds")

head(fc)
head(ka)
    ```

<!-- -->

2.  Take a moment to look at the outputs of Kallisto and featureCounts. Are the rows the same? How can you check? What about the columns?

    ```{r}
head(colnames(fc))
head(rownames(fc))

head(colnames(ka))
head(rownames(ka))

nrow(fc) == nrow(ka)
    ```

<!-- -->

3.  In what format are the counts outputted by kallisto? By featureCounts? What influence could this possibly have on downstream analysis?

    ```{r}
# some code here
    ```

4.  Can you plot the total number of reads per sample?

    ```{r}
  library(dplyr)
library(plotly)
toPlotF <- tibble::tibble(
  Sample=colnames(fc),
  `Read Counts`=colSums(fc)
)
p1 = plot_ly(toPlotF, x = ~Sample, y = ~`Read Counts`, type = "bar") %>% layout(title="Total reads FeatureCounts", yaxis = list(title = "Counts [Mio]"))

toPlotK <- tibble::tibble(
  Sample=colnames(ka),
  `Read Counts`=colSums(ka)
)
p2 = plot_ly(toPlotK, x = ~Sample, y = ~`Read Counts`, type = "bar") %>% layout(title="Total reads Kallisto", yaxis = list(title = "Counts [Mio]"))

p1
p2
    ```

### Excercise #2

Now that the we have generated the counts, we can start exploring the data. We could proceed with either the counts generated by Kallisto or featureCounts. Let's use the featureCounts for now.

So now we have a count matrix of with the following dimensions: 8 x 6'600. In other words, we have represented each sample by a vector of 6'600 features. In order adequately QC our samples, one important aspect is determining how similar or dissimilar one vector of genes from a specific sample is that of another. However, considering 6'600 dimensions directly is not something that humans are generally capable of doing. This is where dimensionality reduction comes in. We would ideally like to *reduce* the given *dimensions* (a 6'600-dimensional space), down to a 2- or 3-dimensional space.

Two approaches, which are in fact related, are PCA (*principal component analysis*) and MDS (*multidimensional scaling*).

Lets load the data and define some basic parameters. I provide you with a code that prepares the data for PCA/MDS - have a look at the code annotation. Do samples cluster according to condition?

```{r}
#Load the data tables
fc <- readRDS("featureCounts/mergedCounts.rds")

# Define meta dataframe for later use
meta <- data.frame(
  Condition=as.factor(rep(c("Glucose", "GlycEth"), each=4)),
  row.names=c(paste0("G", 1:4), paste0("GE", 1:4))
)

# Define some general-use parameters for use later
countDirectoryToUse <- "featureCounts"
sigThresh <- 10
conditionColours <- scales::hue_pal()(length(unique(meta$Condition)))
names(conditionColours) <- unique(meta$Condition)
sampleColours <- conditionColours[meta$Condition]

isPresent <- fc > sigThresh
rawCountsFilt = fc[which(apply(isPresent, 1, any)), ]
rawCountsFilt = round(rawCountsFilt,0)

dds <- DESeq2::DESeqDataSetFromMatrix(countData=rawCountsFilt,
                                      colData=meta,
                                      design=~Condition)
vsd <- DESeq2::vst(dds)

# Extract normalized counts
vsdSE <- SummarizedExperiment::assay(vsd)
```

PCA

PCA is a linear dimensionality reduction technique which, in essence, tries to find a rotation of the data in order to maximise the variance. This is a common method for dimensionality reduction across many different disciplines within Computer Science and beyond. We will use the built-in R-method `prcomp` below to calculate the principal components and manually calculate the variance explained by each component.

Additionally, we will plot a 'scree' plot, which aims to visualise the variance explained by each principal component.

Using the vsdSE object, carry out and plot the PCA and the scree plot.

```{r}
library(tibble)
# Run PCA
pcDat  <- prcomp(t(vsdSE), scale. = FALSE)

# Calculate explained variance
varExp <- (100*pcDat$sdev^2)/sum(pcDat$sdev^2)

# Store the explained variance of top 8 PCs
varExp_df <- data.frame(PC= paste0("PC",1:8),
                          varExp=varExp[1:8])

# Scree plot
varExp_df %>%
  ggplot(aes(x=PC,y=varExp, group=1)) +
  geom_point(colour="steelblue", size=4) +
  geom_col(fill="steelblue") +
  geom_line() + 
  theme_bw() + ylim(c(0,100))

#PCA plot
plot_ly(as.data.frame(pcDat$x), x=~PC1, y=~PC2, color=meta$Condition, colors="Set1",
        type="scatter", mode="markers") %>%
  layout(title="PCA Plot")
```

Multi-dimensional scaling (MDS)

Multi-dimensional scaling is another dimensionality reduction which aims to best reconstruct pairwise distances between a set of points given a set of distances. Since in the case of RNA-seq we are given the data vectors directly rather than the distance matrices, methods must calculate the distance matrix first upon which the MDS algorithm is then performed. PCA is used in the process to produce a reduced dimensionality projection from the similarities.

Here, we use limma [@limma] to calculate the MDS and use plotly to visualise the pairwise distances in 3-dimensions. Use the same vsdSE object.

```{r}
library(tibble)
mds <- limma::plotMDS(vsdSE, plot=FALSE)
mdsOut <- mds$eigen.vectors[,1:3]
colnames(mdsOut) <- c("Leading logFC dim1", "Leading logFC dim2", 
                      "Leading logFC dim3")
toPlot <- cbind(meta %>% rownames_to_column("Sample"), mdsOut)
plot_ly(toPlot, x=~`Leading logFC dim1`, y=~`Leading logFC dim2`, z=~`Leading logFC dim3`, color=~Condition, colors="Set1", type='scatter3d', mode='markers+text', text=~Sample, textposition = "top right") %>%
  plotly::layout(title="Classical MDS", scene=list(xaxis=list(title = 'Leading logFC dim1'), yaxis = list(title = 'Leading logFC dim2'), zaxis = list(title = 'Leading logFC dim3')))
```

### Excercise #3

Correlation Heatmap

Similarly, we can call `pheatmap` on the correlations to easily visualize the similarity between samples within and across conditions. Use the vsdSE object to create a heatmap. Hint: You need to convert the normalized data into the correlation matrix. Use the following function to do so:cor(vsdSE, use="complete.obs"). Do samples cluster according to condition?

```{r}
library(pheatmap)
library(RColorBrewer)
# Pearson correlation plot 
pheatmap(
  mat               = cor(vsdSE, use="complete.obs"),
  treeheight_row    = 100,
  treeheight_col    = 100,
  cutree_rows       = 2, 
  cutree_cols       = 2,
  silent            = F,
  annotation_col    = meta,
  annotation_colors = list(Condition = conditionColours),
  color             = brewer.pal(n = 9, name = "Blues"),
  fontsize_row      = 12, 
  fontsize_col      = 12,
  display_numbers   = TRUE,
  fontsize_number   = 12)
```
